{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "biden = pd.read_csv('Joe_Biden.csv')\n",
    "trump = pd.read_csv('Donald_Trump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hand annotation indicies\n",
    "# Biden\n",
    "positive_biden = [24, 27, 28, 35, 47, 48, 51, 57, 63, 72, 86, 98, 173, 198, 219, 268, 343, 358, 369, 420]\n",
    "\n",
    "negative_biden = [374,156,106,85,171,172,174,186,192,222,229,235,110,6,277,313,356,368,49]\n",
    "\n",
    "neutral_biden = [150,445,161,183,227,250,29,159,240,260,283,305,325,332,341,357,370,380,398,402]\n",
    "\n",
    "# Trump\n",
    "positive_trump = [447,408, 395, 389, 377, 374, 372, 320, 321, 323, 324, 326, 344, 346, 348, 349, 355, 106, 338, 419]\n",
    "\n",
    "negative_trump = [448, 396, 394, 1, 2, 3, 6, 7, 9, 10, 11, 12, 17, 18, 100, 332, 120, 29, 30, 36, 48]\n",
    "\n",
    "neutral_trump = [443, 432, 418, 398, 373, 365, 364, 4, 5, 8, 14, 16, 19, 253, 300, 310, 325, 328, 341, 352]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new col for sentiment\n",
    "\n",
    "biden['sentiment'] = \"\"\n",
    "\n",
    "trump['sentiment'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate rows in new col with corresponding sentiment\n",
    "# biden\n",
    "for i in biden.index:\n",
    "    for j in positive_biden:\n",
    "        if i == j:\n",
    "            biden.at[i,'sentiment'] = 'pos'\n",
    "            \n",
    "for i in biden.index:\n",
    "    for j in neutral_biden:\n",
    "        if i == j:\n",
    "            biden.at[i,'sentiment'] = 'neutral'\n",
    "            \n",
    "for i in biden.index:\n",
    "    for j in negative_biden:\n",
    "        if i == j:\n",
    "            biden.at[i,'sentiment'] = 'neg'\n",
    "\n",
    "            \n",
    "# Trump           \n",
    "for i in trump.index:\n",
    "    for j in positive_trump:\n",
    "        if i == j:\n",
    "            trump.at[i,'sentiment'] = 'pos'\n",
    "            \n",
    "for i in trump.index:\n",
    "    for j in neutral_trump:\n",
    "        if i == j:\n",
    "            trump.at[i,'sentiment'] = 'neutral'\n",
    "            \n",
    "for i in trump.index:\n",
    "    for j in negative_trump:\n",
    "        if i == j:\n",
    "            trump.at[i,'sentiment'] = 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Modeling\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stopwords_english = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select text and news company names\n",
    "biden_sentiment = biden[['text', 'sentiment']]\n",
    "\n",
    "trump_sentiment = trump[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create corpus for each sentiment\n",
    "\n",
    "def create_corpus(biden_sentiment, trump_sentiment, sentiment_name):\n",
    "    \n",
    "    df1 = biden_sentiment.loc[biden_sentiment['sentiment'] == sentiment_name]\n",
    "    df2 = trump_sentiment.loc[trump_sentiment['sentiment'] == sentiment_name]\n",
    "    #df3 = Trump_news.loc[Trump_news['media'] == media_name]\n",
    "    frames = [df1, df2]\n",
    "    df = pd.concat(frames, ignore_index = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small corpus for each sentiment\n",
    "POS = create_corpus(biden_sentiment, trump_sentiment, sentiment_name = 'pos')\n",
    "NEG = create_corpus(biden_sentiment, trump_sentiment, sentiment_name = 'neg')\n",
    "NEUTRAL = create_corpus(biden_sentiment, trump_sentiment, sentiment_name = 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hide highlightingAbstractTranslateUndo Transla...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  Hide highlightingFull TextTranslateUndo Transl...       pos\n",
       "1  Hide highlightingFull TextTranslateUndo Transl...       pos\n",
       "2  Hide highlightingAbstractTranslateUndo Transla...       pos\n",
       "3  Hide highlightingFull TextTranslateUndo Transl...       pos\n",
       "4  Hide highlightingFull TextTranslateUndo Transl...       pos"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_All_sentiment = pd.concat([POS, NEG, NEUTRAL], axis = 0, ignore_index = True)\n",
    "corpus_All_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Preprocessing(corpus):\n",
    "    # convert string to list i.e. ['hide', 'highlightingfull', '[[missing']\n",
    "    corpus['text'] = corpus['text'].str.split()\n",
    "\n",
    "    # lower case each item in the list, and remove non-alphabetic characters i.e. ['hide', 'highlightingfull', 'missing']\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [re.sub(r'[^a-zA-Z]', \"\",y.lower()) for y in x])\n",
    "\n",
    "    # join the item in the list back to a string and replace keywords containing the target names\n",
    "#     keywords = ['new york times', 'the new york times', 'international new york times'\n",
    "#                 \"the washington post\", \"WP Company LLC\", \"washpostcom\",\n",
    "#                 'wall street journal', 'thomaswsjcom', 'Dow Jones Company Inc.']\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [' '.join(x)])\n",
    "\n",
    "    # stem each word in the text\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: str(x[0]))\n",
    "    corpus['text'] = corpus['text'].str.split()\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "\n",
    "    # join the item in the list back to a string\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [' '.join(x)])\n",
    "\n",
    "    # convert list to a string\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: str(x[0]))\n",
    "\n",
    "    print(type(corpus.iloc[0]['text']))\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide highlightingful texttranslateundo transla...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hide highlightingful texttranslateundo transla...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  hide highlightingful texttranslateundo transla...       pos\n",
       "1  hide highlightingful texttranslateundo transla...       pos"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentiment_corpus = Data_Preprocessing(corpus_All_sentiment)\n",
    "processed_sentiment_corpus.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Split training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90    hide highlightingful texttranslateundo transla...\n",
       "48    full texttranslateundo translat fromtotranslat...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate features and targets\n",
    "X = processed_sentiment_corpus.iloc[:, 0]\n",
    "y = processed_sentiment_corpus.iloc[:, 1]\n",
    "\n",
    "# split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0, 'neutral': 1, 'pos': 2}\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "# get label name mapping\n",
    "le.fit(y_train)\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "\n",
    "# encode the target \n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Getting document term matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Create matrix of token counts using unigram, bigram and trigram tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get unigram, bigram, and trigram matrix of token counts\n",
    "\n",
    "def get_DTM(Ngram_range, x_train, x_test):\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df = int(3), max_df = 0.5, \n",
    "                                 ngram_range = Ngram_range, binary=True) \n",
    "    vectorizer.fit(x_train)\n",
    "    trans_x_train = vectorizer.transform(x_train)\n",
    "    trans_x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    return trans_x_train, trans_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram token counts matrix\n",
    "binary1_train, binary1_test = get_DTM(Ngram_range = (1, 1), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# bigram token counts matrix\n",
    "binary2_train, binary2_test = get_DTM(Ngram_range = (1, 2), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# trigram token counts matrix\n",
    "binary3_train, binary3_test = get_DTM(Ngram_range = (1, 3), x_train = X_train, x_test = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique terms in binary1_train is: 2579\n",
      "The unique terms in binary2_train is: 4988\n",
      "The unique terms in binary3_train is: 5735\n"
     ]
    }
   ],
   "source": [
    "print(\"The unique terms in binary1_train is:\", binary1_train.toarray().shape[1])\n",
    "print(\"The unique terms in binary2_train is:\", binary2_train.toarray().shape[1])\n",
    "print(\"The unique terms in binary3_train is:\", binary3_train.toarray().shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Create DTM using unigram, bigram and trigram term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get unigram, bigram, and trigram term frequency matrix\n",
    "\n",
    "def get_TF_DTM(Ngram_range, x_train, x_test):\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df = int(3), max_df = 0.5, ngram_range = Ngram_range) \n",
    "    vectorizer.fit(x_train)\n",
    "    trans_x_train = vectorizer.transform(x_train)\n",
    "    trans_x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    return trans_x_train, trans_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram tf matrix\n",
    "tf1_train, tf1_test = get_TF_DTM(Ngram_range = (1, 1), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# bigram tf matrix\n",
    "tf2_train, tf2_test = get_TF_DTM(Ngram_range = (1, 2), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# trigram tf matrix\n",
    "tf3_train, tf3_test = get_TF_DTM(Ngram_range = (1, 3), x_train = X_train, x_test = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique terms in tf1_train is: 2579\n",
      "The unique terms in tf2_train is: 4988\n",
      "The unique terms in tf3_train is: 5735\n"
     ]
    }
   ],
   "source": [
    "print(\"The unique terms in tf1_train is:\", tf1_train.toarray().shape[1])\n",
    "print(\"The unique terms in tf2_train is:\", tf2_train.toarray().shape[1])\n",
    "print(\"The unique terms in tf3_train is:\", tf3_train.toarray().shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Create DTM using unigram, bigram and trigram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get unigram, bigram, and trigram TF-IDF matrix\n",
    "\n",
    "def get_TF_IDF_DTM(Ngram_range, x_train, x_test):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', min_df = int(3), max_df = 0.5, \n",
    "                                 ngram_range = Ngram_range) \n",
    "    vectorizer.fit(x_train)\n",
    "    trans_x_train = vectorizer.transform(x_train)\n",
    "    trans_x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    return trans_x_train, trans_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unigram tf-idf matrix\n",
    "tfidf1_train, tfidf1_test = get_TF_IDF_DTM(Ngram_range = (1, 1), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# bigram tf-idf matrix\n",
    "tfidf2_train, tfidf2_test = get_TF_IDF_DTM(Ngram_range = (1, 2), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# trigram tf-idf matrix\n",
    "tfidf3_train, tfidf3_test = get_TF_IDF_DTM(Ngram_range = (1, 3), x_train = X_train, x_test = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique terms in tfidf1_train is: 2579\n",
      "The unique terms in tfidf2_train is: 4988\n",
      "The unique terms in tfidf3_train is: 5735\n"
     ]
    }
   ],
   "source": [
    "print(\"The unique terms in tfidf1_train is:\", tfidf1_train.toarray().shape[1])\n",
    "print(\"The unique terms in tfidf2_train is:\", tfidf2_train.toarray().shape[1])\n",
    "print(\"The unique terms in tfidf3_train is:\", tfidf3_train.toarray().shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model training\n",
    "def train_model(clf, dtm, test):\n",
    "    # train data\n",
    "    clf.fit(dtm, y_train)\n",
    "    \n",
    "    # Predicting on the test set\n",
    "    preds = clf.predict(test)\n",
    "    \n",
    "    # print evaluation matrix\n",
    "    print(\"Accuracy:\", '{:1.4f}'.format(accuracy_score(y_test, preds)))\n",
    "    print(\"\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, preds))\n",
    "    \n",
    "    return '{:1.4f}'.format(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram, binary\n",
      "\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "Invalid Parameter format for max_depth expect int but value='{'max_depth': 3, 'eta': 0.3, 'objective': 'multi:softmax', 'num_class': 3}'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0f35f5f89a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"======================================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-ba0ae397321b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(clf, dtm, test)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Predicting on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \"\"\"\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: Invalid Parameter format for max_depth expect int but value='{'max_depth': 3, 'eta': 0.3, 'objective': 'multi:softmax', 'num_class': 3}'"
     ]
    }
   ],
   "source": [
    "# Use Naive Bayes\n",
    "#clf = XGBClassifier() #MultinomialNB()\n",
    "#clf = svm.SVC(gamma = 'scale', C = 1.0)\n",
    "param = {'max_depth': 3, 'eta': 0.3, 'objective':'multi:softmax', 'num_class': 3}\n",
    "xgb_clf = XGBClassifier(param)\n",
    "svm_clf = svm.SVC(gamma = 'scale', C = 1.0)\n",
    "# reference: https://medium.com/@gabrielziegler3/multiclass-multilabel-classification-with-xgboost-66195e4d9f2d\n",
    "# reference: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "# Model Configurations\n",
    "binary1 = (\"unigram, binary\", binary1_train, binary1_test)\n",
    "binary2 = (\"bigram, binary\",  binary2_train, binary2_test)\n",
    "binary3 = (\"trigram, binary\", binary3_train, binary3_test)\n",
    "tf1 = (\"unigram, TF\", tf1_train, tf1_test)\n",
    "tf2 = (\"bigram, TF\",  tf2_train, tf2_test)\n",
    "tf3 = (\"trigram, TF\", tf3_train, tf3_test)\n",
    "tfidf1 = (\"unigram, TF-IDF\", tfidf1_train, tfidf1_test)\n",
    "tfidf2 = (\"bigram, TF-IDF\",  tfidf2_train, tfidf2_test)\n",
    "tfidf3 = (\"trigram, TF-IDF\", tfidf3_train, tfidf3_test)\n",
    "DTMs = [binary1, binary2, binary3,\n",
    "        tf1, tf2, tf3,\n",
    "        tfidf1, tfidf2, tfidf3]\n",
    "\n",
    "df = pd.DataFrame({\"config\": [],\n",
    "                   \"accuracy\": []})\n",
    "best_config = [\"Best Configuration\", \"none\", 0, \"none\", \"none\"]\n",
    "for data in DTMs:\n",
    "    print(data[0])\n",
    "    print(\"\")\n",
    "    score = train_model(clf = xgb_clf, dtm = data[1], test = data[2])\n",
    "    print(\"======================================================\")\n",
    "    print(\"\")\n",
    "    if float(score) > float(best_config[2]):\n",
    "        best_config = [\"Best Configuration:\", data[0], score, data[1], data[2]]\n",
    "    df = df.append({\"config\": data[0],\n",
    "               \"accuracy\": float(score)},\n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram, binary\n",
      "\n",
      "Accuracy: 0.5417\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.88      0.78         8\n",
      "           1       0.50      0.25      0.33         8\n",
      "           2       0.40      0.50      0.44         8\n",
      "\n",
      "    accuracy                           0.54        24\n",
      "   macro avg       0.53      0.54      0.52        24\n",
      "weighted avg       0.53      0.54      0.52        24\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7 0 1]\n",
      " [1 2 5]\n",
      " [2 2 4]]\n",
      "======================================================\n",
      "\n",
      "bigram, binary\n",
      "\n",
      "Accuracy: 0.5833\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.75      0.71         8\n",
      "           1       0.50      0.50      0.50         8\n",
      "           2       0.57      0.50      0.53         8\n",
      "\n",
      "    accuracy                           0.58        24\n",
      "   macro avg       0.58      0.58      0.58        24\n",
      "weighted avg       0.58      0.58      0.58        24\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6 2 0]\n",
      " [1 4 3]\n",
      " [2 2 4]]\n",
      "======================================================\n",
      "\n",
      "trigram, binary\n",
      "\n",
      "Accuracy: 0.5833\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.62      0.62         8\n",
      "           1       0.50      0.62      0.56         8\n",
      "           2       0.67      0.50      0.57         8\n",
      "\n",
      "    accuracy                           0.58        24\n",
      "   macro avg       0.60      0.58      0.58        24\n",
      "weighted avg       0.60      0.58      0.58        24\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 3 0]\n",
      " [1 5 2]\n",
      " [2 2 4]]\n",
      "======================================================\n",
      "\n",
      "unigram, TF\n",
      "\n",
      "Accuracy: 0.4583\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.75      0.63         8\n",
      "           1       0.25      0.12      0.17         8\n",
      "           2       0.44      0.50      0.47         8\n",
      "\n",
      "    accuracy                           0.46        24\n",
      "   macro avg       0.41      0.46      0.42        24\n",
      "weighted avg       0.41      0.46      0.42        24\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6 1 1]\n",
      " [3 1 4]\n",
      " [2 2 4]]\n",
      "======================================================\n",
      "\n",
      "bigram, TF\n",
      "\n",
      "Accuracy: 0.5417\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.62      0.56         8\n",
      "           1       0.44      0.50      0.47         8\n",
      "           2       0.80      0.50      0.62         8\n",
      "\n",
      "    accuracy                           0.54        24\n",
      "   macro avg       0.58      0.54      0.55        24\n",
      "weighted avg       0.58      0.54      0.55        24\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 3 0]\n",
      " [3 4 1]\n",
      " [2 2 4]]\n",
      "======================================================\n",
      "\n",
      "trigram, TF\n",
      "\n",
      "Accuracy: 0.5417\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.62      0.56         8\n",
      "           1       0.44      0.50      0.47         8\n",
      "           2       0.80      0.50      0.62         8\n",
      "\n",
      "    accuracy                           0.54        24\n",
      "   macro avg       0.58      0.54      0.55        24\n",
      "weighted avg       0.58      0.54      0.55        24\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 3 0]\n",
      " [3 4 1]\n",
      " [2 2 4]]\n",
      "======================================================\n",
      "\n",
      "unigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.6250\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.88      0.78         8\n",
      "           1       0.50      0.50      0.50         8\n",
      "           2       0.67      0.50      0.57         8\n",
      "\n",
      "    accuracy                           0.62        24\n",
      "   macro avg       0.62      0.62      0.62        24\n",
      "weighted avg       0.62      0.62      0.62        24\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7 1 0]\n",
      " [2 4 2]\n",
      " [1 3 4]]\n",
      "======================================================\n",
      "\n",
      "bigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.7083\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.88      0.78         8\n",
      "           1       0.60      0.75      0.67         8\n",
      "           2       1.00      0.50      0.67         8\n",
      "\n",
      "    accuracy                           0.71        24\n",
      "   macro avg       0.77      0.71      0.70        24\n",
      "weighted avg       0.77      0.71      0.70        24\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7 1 0]\n",
      " [2 6 0]\n",
      " [1 3 4]]\n",
      "======================================================\n",
      "\n",
      "trigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.6667\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82         8\n",
      "           1       0.56      0.62      0.59         8\n",
      "           2       0.67      0.50      0.57         8\n",
      "\n",
      "    accuracy                           0.67        24\n",
      "   macro avg       0.67      0.67      0.66        24\n",
      "weighted avg       0.67      0.67      0.66        24\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7 1 0]\n",
      " [1 5 2]\n",
      " [1 3 4]]\n",
      "======================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# svm classifier\n",
    "df_svm = pd.DataFrame({\"config\": [],\n",
    "                   \"accuracy\": []})\n",
    "best_config_svm = [\"Best Configuration\", \"none\", 0, \"none\", \"none\"]\n",
    "for data in DTMs:\n",
    "    print(data[0])\n",
    "    print(\"\")\n",
    "    score = train_model(clf = svm_clf, dtm = data[1], test = data[2])\n",
    "    print(\"======================================================\")\n",
    "    print(\"\")\n",
    "    if float(score) > float(best_config_svm[2]):\n",
    "        best_config_svm = [\"Best Configuration:\", data[0], score, data[1], data[2]]\n",
    "    df_svm = df_svm.append({\"config\": data[0],\n",
    "               \"accuracy\": float(score)},\n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unigram, binary</td>\n",
       "      <td>0.5417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bigram, binary</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trigram, binary</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unigram, TF</td>\n",
       "      <td>0.4583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bigram, TF</td>\n",
       "      <td>0.5417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trigram, TF</td>\n",
       "      <td>0.5417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unigram, TF-IDF</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bigram, TF-IDF</td>\n",
       "      <td>0.7083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trigram, TF-IDF</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            config  accuracy\n",
       "0  unigram, binary    0.5417\n",
       "1   bigram, binary    0.5833\n",
       "2  trigram, binary    0.5833\n",
       "3      unigram, TF    0.4583\n",
       "4       bigram, TF    0.5417\n",
       "5      trigram, TF    0.5417\n",
       "6  unigram, TF-IDF    0.6250\n",
       "7   bigram, TF-IDF    0.7083\n",
       "8  trigram, TF-IDF    0.6667"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results of svm classifier\n",
    "df_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Best Configuration:', 'bigram, TF-IDF', '0.7083', <96x4988 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 38749 stored elements in Compressed Sparse Row format>, <24x4988 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 9099 stored elements in Compressed Sparse Row format>]\n"
     ]
    }
   ],
   "source": [
    "# best model\n",
    "print(best_config_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
